{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"balanced.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping={'business':0, 'entertainment':1, 'politics':2, 'sport':3, 'tech':4}\n",
    "df['CategoryId']=df['Category'].map(mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_file=open(\"stopwords.txt\",\"r\",encoding=\"utf-8\")\n",
    "stop_words=stop_words_file.read()\n",
    "stop_words=stop_words.split(\"\\n\")\n",
    "\n",
    "nepali_num_file=open(\"numbers.txt\",\"r\",encoding=\"utf-8\")\n",
    "nepali_num=nepali_num_file.read()\n",
    "nepali_num=nepali_num.split(\",\")\n",
    "\n",
    "nepali_suffix_file=open(\"suffix.txt\",\"r\",encoding=\"utf-8\")\n",
    "nepali_suffix=nepali_suffix_file.read()\n",
    "nepali_suffix=nepali_suffix.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghost\\anaconda3\\envs\\news\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Nepali_nlp import Tokenizer\n",
    "def ProcessText(text):\n",
    "    text=str(text)\n",
    "    \n",
    "    #removing \\n and \\ufeff\n",
    "    remove=['\\n','\\ufeff']\n",
    "    for i in remove:\n",
    "        text.replace(i,'')\n",
    "    \n",
    "    #read stop words\n",
    "    #Remove Stop Words\n",
    "    word_tokens = Tokenizer().word_tokenize(text)\n",
    "    filtered_list = [w for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "    #Remove Nepali numbers\n",
    "    num_filter=[]\n",
    "    for i in range(0,len(filtered_list)):\n",
    "        for j in range(0,len(nepali_num)):\n",
    "            if nepali_num[j] in filtered_list[i]:\n",
    "                num_filter.append(filtered_list[i])\n",
    "                break\n",
    "    for filter in num_filter:\n",
    "        filtered_list.remove(filter)\n",
    "    \n",
    "    #Remove English numbers\n",
    "    num=['0','1','2','3','4','5','6','7','8','9']\n",
    "    num_filter=[]\n",
    "    for i in range(0,len(filtered_list)):\n",
    "        for j in range(0,len(num)):\n",
    "            if num[j] in filtered_list[i]:\n",
    "                num_filter.append(filtered_list[i])\n",
    "                break\n",
    "    for filter in num_filter:\n",
    "        filtered_list.remove(filter)       \n",
    "    \n",
    "    #Stemming Manual\n",
    "    \n",
    "    suffix_filter=filtered_list\n",
    "    '''\n",
    "    for i in range(len(nepali_suffix)):\n",
    "        for j in range(len(suffix_filter)):\n",
    "            if nepali_suffix[i] in suffix_filter[j]:\n",
    "                suffix_removed=suffix_filter[j].split(nepali_suffix[i])[0]\n",
    "                suffix_filter.remove(suffix_filter[j])\n",
    "                suffix_filter.insert(j,suffix_removed)\n",
    "    '''\n",
    "    stemmed_string=' '.join(suffix_filter)\n",
    "    \n",
    "    #stemmed_string=' '.join(filtered_list)\n",
    "    \n",
    "    return suffix_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Process Text in our column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['News']=df['News'].apply(ProcessText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec.load(\"../models/sabda_to_vec_model_md\")\n",
    "test = w2v_model.wv.most_similar('कार्यालय')\n",
    "#vector = model_W2V.wv['प्ल्याबापााकाााा']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "933192"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train,y_test = train_test_split(df['News'],df['Category'],test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(w2v_model.wv.index_to_key)\n",
    "X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "                         for ls in X_train],dtype=object)\n",
    "X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "                         for ls in X_test],dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "655 533\n",
      "734 658\n",
      "842 704\n",
      "267 219\n",
      "564 480\n",
      "353 275\n",
      "445 380\n",
      "281 258\n",
      "655 571\n",
      "236 191\n",
      "246 203\n",
      "165 133\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(X_train_vect):\n",
    "    print(len(X_train.iloc[i]),len(v))\n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_vect_avg = []\n",
    "for v in X_train_vect:\n",
    "    if v.size:\n",
    "        X_train_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
    "        \n",
    "X_test_vect_avg = []\n",
    "for v in X_test_vect:\n",
    "    if v.size:\n",
    "        X_test_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_test_vect_avg.append(np.zeros(100, dtype=float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "655 100\n",
      "734 100\n",
      "842 100\n",
      "267 100\n",
      "564 100\n",
      "353 100\n",
      "445 100\n",
      "281 100\n",
      "655 100\n",
      "236 100\n",
      "246 100\n",
      "165 100\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(X_train_vect_avg):\n",
    "    print(len(X_train.iloc[i]),len(v))\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit RandomForestClassifier On top Of Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_model = rf.fit(X_train_vect_avg, y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf_model.predict(X_test_vect_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.944 / Recall: 0.945 / Accuracy: 0.944\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision = precision_score(y_test, y_pred,average=\"macro\")\n",
    "recall = recall_score(y_test, y_pred,average=\"macro\")\n",
    "print('Precision: {} / Recall: {} / Accuracy: {}'.format(round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['politics'], dtype=object)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"काठमाडौँ — कांग्रेसबाट राष्ट्रपतिका उम्मेदवार रामचन्द्र पौडेलले राष्ट्रिय स्वतन्त्र पार्टी(रास्वपा)सँग समर्थन मागेका छन् । शनिबार दिउँसो रास्वपा सभापति रवि लामिछानेलाई भेटेर उनले आफूलाई समर्थन गर्न माग गरेका हुन् । भेटमा आफू लोकतन्त्रप्रति प्रतिबद्ध रहेको र लामो राजनीतिक यात्रा रहेकाले समर्थन गर्न आग्रह गरेको सभापति लामिछानेले कान्तिपुरलाई बताए । कान्तिपुरसँग संक्षिप्त कुराकानी गर्दै पौडेलसँगको भेटबारे लामिछानेले भने, 'उहाँले राष्ट्रपतिमा समर्थन गर्न आग्रह गर्नुभयो । उहाँले लोकतन्त्रप्रति प्रतिबद्ध छु। लामो राजनीतिक यात्रा छ, तपाईंहरूले समर्थन गर्नु र नगर्नुले ठूलो अर्थ राख्छ भन्नुभयो ।'त्यस्तै लामिछानेले पार्टीले निर्णय गर्नै बाँकी भएको बताएको जानकारी दिए । उनले भने, 'मैले उहाँलाई पार्टीले निर्णय गर्न बाँकी छ पार्टीगत निर्णय गर्छौं भनेँ । तपाईंलाई शुभकामना छ भनेँ ।' रास्वपाले राष्ट्रपति निर्वाचनबारे निर्णय गर्न सोमबार केन्द्रीय समिति बैठक बोलाएको छ । \"\n",
    "\n",
    "text = ProcessText(text)\n",
    "text_vect = np.array([np.array([w2v_model.wv[i] for i in text if i in words])])\n",
    "\n",
    "text_vect_avg = []\n",
    "for v in text_vect:\n",
    "    if v.size:\n",
    "        text_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        text_vect_avg.append(np.zeros(300, dtype=float))\n",
    "rf_model.predict(text_vect_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tech'], dtype=object)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"काठमाडौं । पछिल्लो समय कम्प्युटर तथा स्मार्टफोनको प्रयोग अत्यधिक बढेको छ । जसले गर्दा मानिसहरूले आफ्नो अधिक समय कम्प्युटर तथा स्मार्टफोनको स्क्रिन हेरेर बिताइरहेका  हुन्छन् । एक रिपोर्ट अनुसार विभिन्न देशमा मानिसले दिनमा औसत छ घण्टादेखि १० घण्टासम्मको समय स्क्रिनमा बताउने गरेका छन् ।​ कम्प्यारिटेकका अनुसार अमेरिकामा मात्र दैनिक एक व्यक्तिले औसतमा सात घण्टा चार मिनेट स्क्रिनमा हेरेर बिताउने गरेका छन् । यसरी अत्यधिक समय स्क्रिनमा हेरेर बिताउँदा यसले मानिसको स्वास्थ्यमा प्रतिकूल असर पारेको विभिन्न अध्ययनले देखाएका छन् । आज हामी रिसर्च गेट, हावार्ड युनिभर्सिटी जस्ता उत्कृष्ट संस्थाले तयार पारेको रिपोर्टका आधारमा धेरै समय कम्प्युटर वा मोबाइल स्क्रिनमा बिताउँदा स्वास्थ्यमा कस्ता असर पर्छ भन्ने विषयमा जानकारी दिँदैछौं । कम्प्युटर तथा मोबाइल स्क्रिनमा अधिक समय बिताउँदा किन स्वास्थ्यमा विभिन्न समस्या आउँछन् ?​ स्क्रिनमा अधिक समय बिताउँदा त्यसले आँखालाई कसरी असर गर्छ भन्ने धेरैको जिज्ञासा हुनसक्छ । तल उल्लेख गरिएका कारणले गर्दा स्क्रिनले आँखामा असर गर्छ ।\"\n",
    "\n",
    "text = ProcessText(text)\n",
    "text_vect = np.array([np.array([w2v_model.wv[i] for i in text if i in words])])\n",
    "\n",
    "text_vect_avg = []\n",
    "for v in text_vect:\n",
    "    if v.size:\n",
    "        text_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        text_vect_avg.append(np.zeros(300, dtype=float))\n",
    "rf_model.predict(text_vect_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54d7853ee6fa0f00287d6366039c823ef0d7deed99bd833d3ec41cd5f7adce9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
