{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import gensim.downloader as api\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv(\"BBC News Train.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['business', 'entertainment', 'politics', 'sport', 'tech']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Category_class=sorted(df_train[\"Category\"].unique())\n",
    "Category_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping={'business':0, 'entertainment':1, 'politics':2, 'sport':3, 'tech':4}\n",
    "df_train['CategoryId']=df_train['Category'].map(mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "business         336\n",
       "entertainment    273\n",
       "politics         274\n",
       "sport            346\n",
       "tech             261\n",
       "Name: CategoryId, dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby('Category').CategoryId.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PreProcess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessText(text):\n",
    "    text=str(text)\n",
    "    #lowercasing\n",
    "    text=text.lower()\n",
    "    #Remove Stop Words\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_list = [w for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "    \n",
    "    #Remove numbers and special Symbols\n",
    "    #words like 100m 2m were not removed so using this\n",
    "    num=['0','1','2','3','4','5','6','7','8','9']\n",
    "    num_filter=[]\n",
    "    for i in range(0,len(filtered_list)):\n",
    "        for j in range(0,len(num)):\n",
    "            if num[j] in filtered_list[i]:\n",
    "                num_filter.append(filtered_list[i])\n",
    "                break\n",
    "    \n",
    "    for filter in num_filter:\n",
    "        filtered_list.remove(filter)\n",
    "                \n",
    "    filtered_list = [w for w in filtered_list if w.isalnum()]\n",
    "    filtered_list=  [w for w in filtered_list if not w.isdigit()]\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Lematizing\n",
    "    wordnet_lemmatizer=WordNetLemmatizer()\n",
    "    lemmatized_list=[wordnet_lemmatizer.lemmatize(w,wordnet.VERB) for w in filtered_list]\n",
    "    #lemmatized_string=' '.join(lemmatized_list)\n",
    "    \n",
    "    return lemmatized_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Processed Text in our column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Text_Processed'] = df_train['Text'].apply(ProcessText)\n",
    "df_train['text_clean'] = df_train['Text'].apply(lambda x: gensim.utils.simple_preprocess(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors as wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train,y_test = train_test_split(df_train['Text_Processed'],df_train['Category'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec(X_train , vector_size = 100 , window = 5 , min_count = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(wv.index_to_key )\n",
    "X_train_vect = np.array([np.array([wv[i] for i in ls if i in words])\n",
    "                         for ls in X_train],dtype=object)\n",
    "X_test_vect = np.array([np.array([wv[i] for i in ls if i in words])\n",
    "                         for ls in X_test],dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 67\n",
      "183 178\n",
      "141 132\n",
      "241 237\n",
      "97 96\n",
      "241 233\n",
      "164 156\n",
      "113 108\n",
      "337 321\n",
      "169 156\n",
      "218 195\n",
      "138 138\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(X_train_vect):\n",
    "    print(len(X_train.iloc[i]),len(v))\n",
    "    if i>10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute sentence vectors by averaging the word vectors for the words contained in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_vect_avg = []\n",
    "for v in X_train_vect:\n",
    "    if v.size:\n",
    "        X_train_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_train_vect_avg.append(np.zeros(300, dtype=float))\n",
    "        \n",
    "X_test_vect_avg = []\n",
    "for v in X_test_vect:\n",
    "    if v.size:\n",
    "        X_test_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_test_vect_avg.append(np.zeros(300, dtype=float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are our sentence vector lengths consistent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 300\n",
      "183 300\n",
      "141 300\n",
      "241 300\n",
      "97 300\n",
      "241 300\n",
      "164 300\n",
      "113 300\n",
      "337 300\n",
      "169 300\n",
      "218 300\n",
      "138 300\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(X_train_vect_avg):\n",
    "    print(len(X_train.iloc[i]),len(v))\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc=SVC()\n",
    "knn=KNeighborsClassifier()\n",
    "dtc=DecisionTreeClassifier()\n",
    "mb=MultinomialNB()\n",
    "rtc=RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "models={'svc':svc,'knn':knn,'dtc':dtc,'rtc':rtc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={\n",
    "    'knn': \n",
    "        {'n_neighbors':[3,5,7,9,11,13,15],\n",
    "        'metric':['cosine','euclidean','manhattan'],\n",
    "        'weights':['uniform','distance']},\n",
    "    \n",
    "    'svc': {'C':[0.1,1,10,100], \n",
    "            'gamma':[1,0.1,0.01,0.01], \n",
    "            'kernel':['rbf','linear']},\n",
    "    \n",
    "    'dtc':{\n",
    "        'criterion':['gini','entropy'],\n",
    "        'max_depth':[2,4,6,8,10,12]\n",
    "    },\n",
    "    \n",
    "    'mb':{\n",
    "        'alpha': [1.0,2.0],\n",
    "    'fit_prior': [True]\n",
    "    },\n",
    "    'rtc':{\n",
    "        'criterion':['gini','entropy'],\n",
    "        'max_depth':[2,4,6,8,10,12]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc :  SVC(C=10, gamma=1, kernel='linear')\n",
      "knn :  KNeighborsClassifier(metric='cosine', weights='distance')\n",
      "dtc :  DecisionTreeClassifier(criterion='entropy', max_depth=12)\n",
      "rtc :  RandomForestClassifier(max_depth=10)\n"
     ]
    }
   ],
   "source": [
    "model_accuracy={}\n",
    "score=0.0001\n",
    "for model in models.keys():\n",
    "    mod = GridSearchCV(\n",
    "    models[model],\n",
    "    params[model],\n",
    "    verbose=0, #Progress bar showing\n",
    "    cv=5, #cross validation\n",
    "    n_jobs=-1, #cores to assign\n",
    ")\n",
    "    gridsearch_result=mod.fit(X_train_vect_avg,y_train.values.ravel())\n",
    "    \n",
    "    #selects best model\n",
    "    if(score < float(gridsearch_result.score(X_test_vect_avg,y_test))):\n",
    "            score=gridsearch_result.score(X_test_vect_avg,y_test)\n",
    "            best_model=gridsearch_result\n",
    "            \n",
    "    predict=mod.predict(X_test_vect_avg)\n",
    "    print(f\"{model} : \",gridsearch_result.best_estimator_)\n",
    "    if model not in model_accuracy.keys():\n",
    "        model_accuracy.update({model:accuracy_score(y_test,predict)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svc': 0.9731543624161074,\n",
       " 'knn': 0.9530201342281879,\n",
       " 'dtc': 0.8120805369127517,\n",
       " 'rtc': 0.9496644295302014}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier\n",
    "# rf = AdaBoostClassifier(n_estimators=100, learning_rate=0.5, random_state=0)\n",
    "rf = GradientBoostingClassifier(n_estimators=100, learning_rate=.5, max_depth=1, random_state=0)\n",
    "\n",
    "#rf = RandomForestClassifier()\n",
    "rf_model = rf.fit(X_train_vect_avg,y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['tech', 'politics', 'tech', 'sport', 'tech', 'sport', 'sport',\n",
       "       'entertainment', 'politics', 'sport', 'tech', 'tech',\n",
       "       'entertainment', 'tech', 'tech', 'business', 'tech', 'tech',\n",
       "       'sport', 'politics', 'tech', 'business', 'entertainment',\n",
       "       'business', 'tech', 'business', 'entertainment', 'sport',\n",
       "       'politics', 'sport', 'politics', 'politics', 'politics', 'sport',\n",
       "       'entertainment', 'tech', 'politics', 'entertainment', 'tech',\n",
       "       'entertainment', 'tech', 'entertainment', 'tech', 'politics',\n",
       "       'business', 'politics', 'business', 'business', 'tech', 'sport',\n",
       "       'sport', 'sport', 'sport', 'politics', 'entertainment',\n",
       "       'entertainment', 'business', 'sport', 'entertainment',\n",
       "       'entertainment', 'entertainment', 'sport', 'business',\n",
       "       'entertainment', 'business', 'tech', 'business', 'business',\n",
       "       'entertainment', 'business', 'business', 'tech', 'politics',\n",
       "       'business', 'tech', 'sport', 'business', 'tech', 'tech', 'sport',\n",
       "       'business', 'politics', 'politics', 'politics', 'entertainment',\n",
       "       'politics', 'entertainment', 'politics', 'sport', 'entertainment',\n",
       "       'sport', 'entertainment', 'entertainment', 'entertainment',\n",
       "       'business', 'entertainment', 'politics', 'tech', 'entertainment',\n",
       "       'entertainment', 'politics', 'tech', 'entertainment', 'tech',\n",
       "       'politics', 'politics', 'politics', 'sport', 'tech', 'tech',\n",
       "       'business', 'business', 'business', 'business', 'politics',\n",
       "       'politics', 'politics', 'entertainment', 'tech', 'politics',\n",
       "       'politics', 'politics', 'entertainment', 'politics',\n",
       "       'entertainment', 'entertainment', 'politics', 'business', 'sport',\n",
       "       'business', 'business', 'tech', 'politics', 'entertainment',\n",
       "       'tech', 'politics', 'sport', 'tech', 'entertainment', 'sport',\n",
       "       'politics', 'business', 'business', 'tech', 'entertainment',\n",
       "       'politics', 'business', 'sport', 'sport', 'entertainment',\n",
       "       'entertainment', 'politics', 'business', 'business', 'tech',\n",
       "       'sport', 'entertainment', 'tech', 'sport', 'business', 'business',\n",
       "       'politics', 'politics', 'tech', 'tech', 'business', 'politics',\n",
       "       'sport', 'business', 'tech', 'tech', 'tech', 'entertainment',\n",
       "       'sport', 'politics', 'business', 'sport', 'sport', 'politics',\n",
       "       'entertainment', 'entertainment', 'business', 'sport', 'sport',\n",
       "       'business', 'entertainment', 'sport', 'sport', 'sport',\n",
       "       'entertainment', 'tech', 'tech', 'politics', 'sport', 'business',\n",
       "       'entertainment', 'business', 'business', 'entertainment',\n",
       "       'business', 'politics', 'entertainment', 'business', 'politics',\n",
       "       'entertainment', 'business', 'entertainment', 'politics', 'sport',\n",
       "       'entertainment', 'sport', 'entertainment', 'tech', 'entertainment',\n",
       "       'sport', 'sport', 'sport', 'entertainment', 'business', 'sport',\n",
       "       'politics', 'politics', 'sport', 'politics', 'business', 'sport',\n",
       "       'tech', 'tech', 'business', 'politics', 'sport', 'tech',\n",
       "       'entertainment', 'politics', 'entertainment', 'entertainment',\n",
       "       'politics', 'politics', 'tech', 'sport', 'business', 'politics',\n",
       "       'politics', 'tech', 'business', 'business', 'entertainment',\n",
       "       'tech', 'business', 'sport', 'sport', 'sport', 'tech', 'sport',\n",
       "       'business', 'entertainment', 'politics', 'business', 'politics',\n",
       "       'business', 'entertainment', 'politics', 'business',\n",
       "       'entertainment', 'business', 'business', 'sport', 'tech',\n",
       "       'entertainment', 'politics', 'business', 'tech', 'business',\n",
       "       'sport', 'tech', 'politics', 'tech', 'tech', 'entertainment',\n",
       "       'business', 'entertainment', 'entertainment', 'business', 'sport',\n",
       "       'business', 'business', 'tech', 'sport', 'entertainment',\n",
       "       'entertainment', 'entertainment', 'politics', 'business',\n",
       "       'business', 'sport', 'entertainment', 'sport', 'entertainment'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rf_model.predict(X_test_vect_avg)\n",
    "y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.947 / Recall: 0.946 / Accuracy: 0.946\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision = precision_score(y_test, y_pred,average=\"weighted\")\n",
    "recall = recall_score(y_test, y_pred,average=\"weighted\")\n",
    "print('Precision: {} / Recall: {} / Accuracy: {}'.format(round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['politics'], dtype=object)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"A secretariat meeting of the party held on Monday morning decided to quit the Pushpa Kamal Dahal-led government and also withdraw its support to the government. The decision by the party comes in the wake of a changed political equation in the run-up to the presidential election. UML had earlier decided to wait till the presidential election scheduled for March 9 before taking a final call on whether to leave the government. Bishnu Paudel, UML’s vice-chairman, said the meeting decided to pull out of the government after Prime Minister Pushpa Kamal Dahal ‘started working in a different fashion’. Prime minister directed the foreign minister to cancel her foreign trip. It did not seem appropriate for us to stay in the government after he asked to leave the government voluntarily or work as a minister without portfolio,” said Paudel talking to journalists after the meeting.The party, riled by the prime minister’s direction to Minister for Foreign Affairs Bimala Rai Paudyal to cancel her Geneva trip, had summoned the secretariat meeting. \"\n",
    "text = ProcessText(text)\n",
    "text_vect = np.array([np.array([wv[i] for i in text if i in words])])\n",
    "\n",
    "text_vect_avg = []\n",
    "for v in text_vect:\n",
    "    if v.size:\n",
    "        text_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        text_vect_avg.append(np.zeros(300, dtype=float))\n",
    "rf_model.predict(text_vect_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sport'], dtype=object)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Nepal has been confirmed as the host nation of the first round of the Women's Olympic Football Asian Qualifiers for the Paris Olympics in 2024, ANFA said on its website on Sunday. Nepal have been drawn alongside Vietnam, Palestine and Afghanistan in Group C. Palestine will not travel to Nepal after they withdrew from the tournament. “AFC confirmed Nepal as the host nation for Group C,” ANFA added. The first round matches will be held from April 3 and 11. Seven group winners from the first round will join North Korea, Australia, China, Japan and South Korea—who received bye as the five-highest ranked teams—in the second round. The 12 teams will be drawn into three groups in the second round, with the winner of each group and one best runner-up advancing to the third round. The top two teams from the third round will secure their berths in the Paris Olympics 2024.\"\n",
    "\n",
    "text = ProcessText(text)\n",
    "text_vect = np.array([np.array([wv[i] for i in text if i in words])])\n",
    "\n",
    "text_vect_avg = []\n",
    "for v in text_vect:\n",
    "    if v.size:\n",
    "        text_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        text_vect_avg.append(np.zeros(300, dtype=float))\n",
    "rf_model.predict(text_vect_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54d7853ee6fa0f00287d6366039c823ef0d7deed99bd833d3ec41cd5f7adce9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
